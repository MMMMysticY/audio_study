{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2-1 CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = []\n",
    "label_ids = []\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CWS_dataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_map = {label: i for i, label in enumerate(['B','M','E','S','O'])}\n",
    "        \n",
    "        with open(data_path , 'r') as f:\n",
    "            data = f.readlines()\n",
    "            \n",
    "            sentence, labels = [], []\n",
    "\n",
    "            string, output = ['[CLS]'], [4]\n",
    "            for w in data:\n",
    "                if w != '\\n':\n",
    "                    string.append(w[:w.find(\"\\t\")])\n",
    "                    output.append(self.label_map[w[-2]])\n",
    "                elif w == '\\n':\n",
    "                    sentence.append(string+['[SEP]']), labels.append(output+[4])\n",
    "                    string, output = ['[CLS]'], [4]\n",
    "                                \n",
    "            self.data = {'sentence':sentence, 'labels':labels}\n",
    "            assert len(self.data['sentence'])==len(self.data['labels']) \n",
    "    \n",
    "    def __len__(self):\n",
    "        assert len(self.data['sentence'])==len(self.data['labels']) \n",
    "        return len(self.data['sentence'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data['sentence'][idx], self.data['labels'][idx]\n",
    "    \n",
    "    def collate_fn(self, samples):\n",
    "        tokenizer = self.tokenizer\n",
    "        \n",
    "        b_token_ids = []\n",
    "        b_label_ids = []\n",
    "        \n",
    "        ## word to token id\n",
    "        for sentence, labels in samples:\n",
    "            token_ids, label_ids = [], []\n",
    "            for index, word in enumerate(sentence):\n",
    "                ids = tokenizer.convert_tokens_to_ids(word)\n",
    "                \n",
    "                token_ids.append(ids)\n",
    "                label_ids.append(labels[index])\n",
    "                \n",
    "            assert len(token_ids)==len(label_ids)\n",
    "                \n",
    "            b_token_ids.append(token_ids)\n",
    "            b_label_ids.append(label_ids)\n",
    "            \n",
    "        ## fix to same length \n",
    "        max_length = max([len(token_ids) for token_ids in b_token_ids])\n",
    "        max_length = 512 if max_length>512 else max_length\n",
    "        \n",
    "        PAD_token = tokenizer.convert_tokens_to_ids('[PAD]')\n",
    "        for token_ids, label_ids in zip(b_token_ids, b_label_ids):\n",
    "            token_ids += [PAD_token]*(max_length-len(token_ids))\n",
    "            label_ids += [4]*(max_length-len(label_ids))\n",
    "        \n",
    "        return torch.tensor(b_token_ids), torch.tensor(b_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import torch\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "# model_config = BertConfig(vocab_size=tokenizer.vocab_size, hidden_size=768, num_hidden_layers=4, num_labels=5)\n",
    "# #model = BertForTokenClassification(config=model_config)\n",
    "# model = BertForTokenClassification.from_pretrained('bert-base-chinese', num_labels=5)\n",
    "# model.config.save_pretrained('output')  #save config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def metrics(output, label):\n",
    "    output = torch.max(output, 1)[1]\n",
    "    acc = ((output==label).int()*(label!=4).int()).sum().item()/((label>=0).int()*(label!=4).int()).sum().item()\n",
    "    return acc\n",
    "\n",
    "def _run_train(args, model, criterion, optimizer, dataloader):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    total_loss, total_acc = 0, 0\n",
    "    for idx, (b_token_ids, b_label_ids) in enumerate(dataloader):\n",
    "        b = b_token_ids.shape[0]\n",
    "                \n",
    "        output = model(b_token_ids.cuda())[0]\n",
    "        \n",
    "        output = output.view(-1,5)\n",
    "        b_label_ids = b_label_ids.cuda().view(-1)\n",
    "        \n",
    "        ## calcu loss\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, b_label_ids)\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()*b\n",
    "        total_acc += metrics(output, b_label_ids)*b\n",
    "        print(\"\\t[{}/{}] train loss:{:.3f}, acc:{:.3f} \".format(\n",
    "                                            idx+1,\n",
    "                                            len(dataloader),\n",
    "                                            total_loss/(idx+1)/b,\n",
    "                                            total_acc/(idx+1)/b),\n",
    "                                    end='   \\r')     \n",
    "\n",
    "    return total_loss/len(dataloader.dataset), total_acc/len(dataloader.dataset)\n",
    "\n",
    "def _run_valid(args, model, criterion, dataloader):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        total_loss, total_acc = 0, 0\n",
    "        for idx, (b_token_ids, b_label_ids) in enumerate(dataloader):\n",
    "            b = b_token_ids.shape[0]\n",
    "\n",
    "            output = model(b_token_ids.cuda())[0]\n",
    "\n",
    "            output = output.view(-1,5)\n",
    "            b_label_ids = b_label_ids.cuda().view(-1)\n",
    "\n",
    "            ## calcu loss\n",
    "            loss = criterion(output, b_label_ids)\n",
    "\n",
    "            total_loss += loss.item()*b\n",
    "            total_acc += metrics(output, b_label_ids)*b\n",
    "            print(\"\\t[{}/{}] valid loss:{:.3f}, acc:{:.3f} \".format(\n",
    "                                                idx+1,\n",
    "                                                len(dataloader),\n",
    "                                                total_loss/(idx+1)/b,\n",
    "                                                total_acc/(idx+1)/b),\n",
    "                                        end='   \\r')     \n",
    "\n",
    "        return total_loss/len(dataloader.dataset), total_acc/len(dataloader.dataset) \n",
    "\n",
    "def train(args, train_dataloader, valid_dataloader):\n",
    "    torch.manual_seed(987)\n",
    "    torch.cuda.manual_seed(987)\n",
    "    \n",
    "    #if os.path.isfile(args.load_path):\n",
    "        #model.load_state_dict(torch.load(args.load_model)['state_dict'])\n",
    "    model.cuda() \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=4).cuda()\n",
    "     \n",
    "    optimizer = torch.optim.AdamW(list(model.parameters()), \n",
    "                                  lr  = args.lr,\n",
    "                                  eps = 1e-8 )\n",
    "    \n",
    "    best_loss = 100\n",
    "    for epoch in range(1,args.epoch+1):\n",
    "        print(f' Epoch {epoch}')\n",
    "            \n",
    "        loss, acc = _run_train(args, model, criterion, optimizer, train_dataloader)\n",
    "        print(\"\\t[Info] Train avg loss:{:.4f}, acc:{:.4f}\".format(loss,acc))\n",
    "        \n",
    "        loss, acc = _run_valid(args, model, criterion, valid_dataloader)\n",
    "        print(\"\\t[Info] Valid avg loss:{:.4f}, acc:{:.4f}\".format(loss,acc))\n",
    "        \n",
    "        ## Save best model\n",
    "        if loss < best_loss:\n",
    "            #torch.save({'state_dict': model.state_dict()}, \"{}/model_best.pt\".format(args.save_path))\n",
    "            model.save_pretrained(\"{}/\".format(args.save_path))\n",
    "            print('\\t[Info] save weights best')\n",
    "        ## Save newest model\n",
    "        #torch.save({'state_dict': model.state_dict()}, \"{}/model.pt\".format(args.save_path))\n",
    "        model.save_pretrained(\"{}/\".format(args.save_path))\n",
    "        print('\\t[Info] save weights {}epoch'.format(epoch))\n",
    "      \n",
    "        ## Update learning rate\n",
    "        '''\n",
    "        if epoch>2:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] /= 3\n",
    "                if param_group['lr'] < 1e-6:\n",
    "                    param_group['lr'] = 1e-6 \n",
    "        '''\n",
    "        print('\\t--------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.22) or chardet (2.1.1) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1\n",
      "\t[Info] Train avg loss:0.0957, acc:0.9672   \n",
      "\t[Info] Valid avg loss:0.0343, acc:0.9881 \n",
      "\t[Info] save weights best\n",
      "\t[Info] save weights 1epoch\n",
      "\t--------------------------------------------------------\n",
      " Epoch 2\n",
      "\t[Info] Train avg loss:0.0419, acc:0.9854   \n",
      "\t[Info] Valid avg loss:0.0202, acc:0.9931 \n",
      "\t[Info] save weights best\n",
      "\t[Info] save weights 2epoch\n",
      "\t--------------------------------------------------------\n",
      " Epoch 3\n",
      "\t[Info] Train avg loss:0.0257, acc:0.9909   \n",
      "\t[Info] Valid avg loss:0.0127, acc:0.9958 \n",
      "\t[Info] save weights best\n",
      "\t[Info] save weights 3epoch\n",
      "\t--------------------------------------------------------\n",
      "finish~\n"
     ]
    }
   ],
   "source": [
    "import os, argparse\n",
    "\n",
    "from transformers import BertTokenizer, BertForTokenClassification, BertConfig\n",
    "\n",
    "def parse_args(string=\"\"):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--lr', default=2e-5,\n",
    "                            type=float, help='leanring rate')\n",
    "    parser.add_argument('--epoch', default=3,\n",
    "                            type=int, help='epochs')\n",
    "    parser.add_argument('--batch-size', default=8,\n",
    "                            type=int, help='batch size')\n",
    "    parser.add_argument('--gpu', default=\"0\",\n",
    "                            type=str, help=\"0:1080ti 1:1070\")\n",
    "    parser.add_argument('--num-workers', default=8,\n",
    "                            type=int, help='dataloader num workers')\n",
    "    parser.add_argument('--save-path', default='OUT_DIR',\n",
    "                            type=str, help='.pth model file save dir')\n",
    "    parser.add_argument('--load-path', default='trained_model/my_cws_bert.pt',\n",
    "                            type=str, help='.pth model file save dir')\n",
    "    args = parser.parse_args(string)\n",
    "    return args\n",
    "\n",
    "if __name__=='__main__':\n",
    "    args = parse_args()\n",
    "    \n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu #0:1080ti 1:1070\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "    \n",
    "    #model_config = BertConfig(vocab_size=tokenizer.vocab_size, hidden_size=768, num_hidden_layers=4, num_labels=5)\n",
    "    #model = BertForTokenClassification(config=model_config)\n",
    "    model = BertForTokenClassification.from_pretrained('bert-base-chinese', num_labels=5)\n",
    "    model.config.save_pretrained(args.save_path)  #save config\n",
    "\n",
    "    train_dataset = CWS_dataset('/media/D/DLHLP/hw4/CWS_dlhlp/train_proc.txt', tokenizer)\n",
    "    valid_dataset = CWS_dataset('/media/D/DLHLP/hw4/CWS_dlhlp/dev_proc.txt', tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, \n",
    "                                  batch_size = args.batch_size,\n",
    "                                  num_workers= args.num_workers,                              \n",
    "                                  collate_fn = train_dataset.collate_fn,\n",
    "                                  shuffle = True)\n",
    "    valid_dataloader = DataLoader(train_dataset, \n",
    "                                  batch_size = args.batch_size*3,\n",
    "                                  num_workers= args.num_workers,\n",
    "                                  collate_fn = train_dataset.collate_fn)\n",
    "\n",
    "    train(args, train_dataloader, valid_dataloader)\n",
    "    \n",
    "    print(\"finish~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我,一直,親自,指揮,親自,部署,我,相信,只要,我們,堅定,信心,同舟共濟,科學,防治,精準,施策,我們,一定,會,戰勝,這,一,次,疫情\n",
      "這,個,聲明,讓,我,再次,想起,了,安徒生,的,童話,皇帝,的,新裝\n",
      "希望,他們,能夠,聽,一,聽,這,個,忠告,不,要,再,信口雌黃,地,抹黑,居心叵測,地,挑撥,煞有介事,地,恫嚇\n",
      "有關,部門,當然,就,是,有關,的,部門,了,無關,的,就,不,能,稱為,有關,部門,所以,我,建議,你,還是,要,向,他們,詢問\n",
      "不,要,搞,奇奇怪怪,的,建築\n",
      "現在,提請,表決,同意,的,代表,請,舉手,請,放下,不,同意,的,請,舉手,沒有,棄權,的,請,舉手,沒有,通過\n",
      "輕關,易道,通商,寬衣\n",
      "人均,國內,生產,總值,接近,八千萬,美元\n",
      "我,青年,時代,就,對,法國,文化,抱有,濃厚,興趣,法國,的,歷史,哲學,文學,藝術,深深,吸引,著,我,讀,法國,近現代史,特別是,法國,大,革命史,的,書籍,讓,我,豐富,了,對,人類,社會,政治,演進,規律,的,思考,讀,孟德斯鳩伏爾泰盧梭狄德羅,聖西門傅立葉薩特,等,人,的,著作,讓,我,加深,了,對,思想,進步,對,人類,社會,進步,作用,的,認識,讀,蒙田拉封丹莫里哀司湯達巴爾扎克,雨果,大仲馬,喬治·桑福樓拜,小,仲馬,莫泊桑羅曼·羅蘭,等,人,的,著作,讓,我,增加,了,對,人類,生活,中,悲歡離合,的,感觸,冉阿讓卡西莫多,羊脂球,等,藝術,形象,至今,仍,栩栩如生,地,存在,於,我,的,腦海,之中,欣賞,米勒馬奈德加塞尚莫內羅丹,等,人,的,藝術,作品,以及,趙無極,中,西,合璧,的,畫作,讓,我,提升,了,自己,的,藝術,鑑賞,能力,還有,讀,凡爾納,的,科幻,小說,讓,我,的,頭腦,充滿,了,無盡,的,想像\n",
      "因為,我,那,時候,扛,兩百,斤,麥子,十,里,山路,不,換肩,的\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, BertForTokenClassification, BertConfig\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "config = BertConfig.from_pretrained('OUT_DIR/config.json')\n",
    "model = BertForTokenClassification.from_pretrained('OUT_DIR/pytorch_model.bin', config=config).cuda()#, config=config).cuda()\n",
    "\n",
    "report_sentence = ['我一直親自指揮、親自部署，我相信只要我們堅定信心、同舟共濟、科學防治、精準施策，我們一定會戰勝這一次疫情。',\n",
    "                   '這個聲明讓我再次想起了安徒生的童話《皇帝的新裝》。',\n",
    "                   '希望他們能夠聽一聽這個忠告，不要再信口雌黃地抹黑，居心叵測地挑撥，煞有介事地恫嚇。',\n",
    "                   '有關部門當然就是有關的部門了。無關的就不能稱為有關部門。所以我建議你還是要向他們詢問。',\n",
    "                   '不要搞奇奇怪怪的建築。',\n",
    "                   '現在提請表決。同意的代表請舉手。請放下；不同意的請舉手。沒有；棄權的請舉手。沒有。通過！',\n",
    "                   '輕關易道，通商寬衣。',\n",
    "                   '人均國內生產總值接近八千萬美元。',\n",
    "                   '我青年時代就對法國文化抱有濃厚興趣，法國的歷史、哲學、文學、藝術深深吸引著我。讀法國近現代史特別是法國大革命史的書籍，讓我豐富了對人類社會政治演進規律的思考。讀孟德斯鳩、伏爾泰、盧梭、狄德羅、聖西門、傅立葉、薩特等人的著作，讓我加深了對思想進步對人類社會進步作用的認識。讀蒙田、拉封丹、莫里哀、司湯達、巴爾扎克、雨果、大仲馬、喬治·桑、福樓拜、小仲馬、莫泊桑、羅曼·羅蘭等人的著作，讓我增加了對人類生活中悲歡離合的感觸。冉阿讓、卡西莫多、羊脂球等藝術形象至今仍栩栩如生地存在於我的腦海之中。欣賞米勒、馬奈、德加、塞尚、莫內、羅丹等人的藝術作品，以及趙無極中西合璧的畫作，讓我提升了自己的藝術鑑賞能力。還有，讀凡爾納的科幻小說，讓我的頭腦充滿了無盡的想像。',\n",
    "                   '因為我那時候，扛兩百斤麥子，十里山路不換肩的。']\n",
    "\n",
    "rm_notate = True\n",
    "if rm_notate:\n",
    "    for idx,sent in enumerate(report_sentence):\n",
    "        for c in \"，、!。！。;；《》\":\n",
    "            sent = sent.replace(c,\"\")\n",
    "        report_sentence[idx] = sent\n",
    "\n",
    "label_map = {label: i for i, label in enumerate(['B','M','E','S','O'])}\n",
    "\n",
    "output_sentence = []\n",
    "\n",
    "for sentence in report_sentence:\n",
    "\n",
    "    # 標記\n",
    "    token_ids = tokenizer.encode(sentence)\n",
    "\n",
    "    model.eval()\n",
    "    label = model(torch.tensor([token_ids]).cuda())[0]\n",
    "    label = torch.max(label[:,1:-1],2)[1].cpu().numpy().tolist()[0]\n",
    "    \n",
    "    # 斷詞\n",
    "    assert len(sentence)==len(label)\n",
    "    seg_sentence = ''\n",
    "    for l,w in zip(label,sentence):\n",
    "        if l==label_map['B']:\n",
    "            seg_sentence += f'{w}'\n",
    "        elif l==label_map['E']:\n",
    "            seg_sentence += f'{w},'\n",
    "        elif l==label_map['M']:\n",
    "            seg_sentence += f'{w}'\n",
    "        elif l==label_map['S']:\n",
    "            seg_sentence += f'{w},'\n",
    "        \n",
    "    output_sentence.append(seg_sentence[:-1])\n",
    "\n",
    "for sent in output_sentence:\n",
    "    print(sent)\n",
    "    continue\n",
    "    \n",
    "with open('segmented.csv', 'w') as f:\n",
    "    for sent in output_sentence:\n",
    "        f.write(sent)\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## with notation\n",
    "我,一直,親自,指揮,、,親自,部署,，,我,相信,只要,我們,堅定,信心,、,同舟共濟,、,科學,防治,、,精準,施策,，,我們,一定,會,戰勝,這,一,次,疫情,。\n",
    "這,個,聲明,讓,我,再次,想起,了,安徒生,的,童話,《,皇帝,的,新裝,》,。\n",
    "希望,他們,能夠,聽,一,聽,這,個,忠告,，,不,要,再,信口雌黃,地,抹黑,，,居心叵測,地,挑撥,，,煞有介事,地,恫嚇,。\n",
    "有關,部門,當然,就,是,有關,的,部門,了,。,無關,的,就,不,能,稱為,有關,部門,。,所以,我,建議,你,還是,要,向,他們,詢問,。\n",
    "不,要,搞,奇奇怪怪,的,建築,。\n",
    "現在,提請,表決,。,同意,的,代表,請,舉手,。,請,放下,；,不,同意,的,請,舉手,。,沒有,；,棄權,的,請,舉手,。,沒有,。,通過,！\n",
    "輕關易道,，,通商,寬衣,。\n",
    "人均,國內,生產,總值,接近,八千萬,美元,。\n",
    "我,青年,時代,就,對,法國,文化,抱有,濃厚,興趣,，,法國,的,歷史,、,哲學,、,文學,、,藝術,深深,吸引,著,我,。,讀,法國,近現代史,特別是,法國,大,革命史,的,書籍,，,讓,我,豐富,了,對,人類,社會,政治,演進,規律,的,思考,。,讀,孟德斯鳩,、,伏爾泰,、,盧梭,、,狄德羅,、,聖西門,、,傅立葉,、,薩特,等,人,的,著作,，,讓,我,加深,了,對,思想,進步,對,人類,社會,進步,作用,的,認識,。,讀,蒙田,、,拉封丹,、,莫里哀,、,司湯達,、,巴爾扎克,、,雨果,、,大仲馬,、,喬治·桑,、,福樓拜,、,小仲馬,、,莫泊桑,、,羅曼·羅蘭,等,人,的,著作,，,讓,我,增加,了,對,人類,生活,中,悲歡離合,的,感觸,。,冉阿讓,、,卡西莫多,、,羊脂球,等,藝術,形象,至今,仍,栩栩如生,地,存在,於,我,的,腦海,之中,。,欣賞,米勒,、,馬奈,、,德加,、,塞尚,、,莫內,、,羅丹,等,人,的,藝術,作品,，,以及,趙無極,中,西,合璧,的,畫作,，,讓,我,提升,了,自己,的,藝術,鑑賞,能力,。,還有,，,讀,凡爾納,的,科幻,小說,，,讓,我,的,頭腦,充滿,了,無盡,的,想像,。\n",
    "因為,我,那,時候,，,扛,兩百,斤,麥子,，,十里山路,不,換肩,的,。\n",
    "\n",
    "## without notation\n",
    "我,一直,親自,指揮,親自,部署,我,相信,只要,我們,堅定,信心,同舟共濟,科學,防治,精準,施策,我們,一定,會,戰勝,這,一,次,疫情\n",
    "這,個,聲明,讓,我,再次,想起,了,安徒生,的,童話,《,皇帝,的,新裝,》\n",
    "希望,他們,能夠,聽,一,聽,這,個,忠告,不,要,再,信口雌黃,地,抹黑,居心叵測,地,挑撥,煞有介事,地,恫嚇\n",
    "有關,部門,當然,就,是,有關,的,部門,了,無關,的,就,不,能,稱為,有關,部門,所以,我,建議,你,還是,要,向,他們,詢問\n",
    "不,要,搞,奇奇怪怪,的,建築\n",
    "現在,提請,表決,同意,的,代表,請,舉手,請,放下,；,不,同意,的,請,舉手,沒有,；,棄權,的,請,舉手,沒有,通過,！\n",
    "輕關,易道,通商,寬衣\n",
    "人均,國內,生產,總值,接近,八千萬,美元\n",
    "我,青年,時代,就,對,法國,文化,抱有,濃厚,興趣,法國,的,歷史,哲學,文學,藝術,深深,吸引,著,我,讀,法國,近現代史,特別是,法國,大,革命史,的,書籍,讓,我,豐富,了,對,人類,社會,政治,演進,規律,的,思考,讀,孟德斯鳩,伏爾泰盧梭狄德羅聖西門傅立葉薩特,等,人,的,著作,讓,我,加深,了,對,思想,進步,對,人類,社會,進步,作用,的,認識,讀,蒙田拉封丹,莫里哀司湯達巴爾扎克,雨果,大仲馬,喬治·桑福樓拜,小仲馬,莫泊桑羅曼·羅蘭,等,人,的,著作,讓,我,增加,了,對,人類,生活,中,悲歡離合,的,感觸,冉阿讓卡西莫多,羊脂球,等,藝術,形象,至今,仍,栩栩如生,地,存在,於,我,的,腦海,之中,欣賞,米勒馬奈德加塞尚莫內羅丹,等,人,的,藝術,作品,以及,趙無極,中,西,合璧,的,畫作,讓,我,提升,了,自己,的,藝術,鑑賞,能力,還有,讀,凡爾納,的,科幻,小說,讓,我,的,頭腦,充滿,了,無盡,的,想像\n",
    "因為,我,那,時候,扛,兩百,斤,麥子,十里山路,不,換肩,的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
