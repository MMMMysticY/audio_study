# End-to-end-ASR-Pytorch
这是课程作业推荐使用的ASR原型系统 这样就不用自己实现了

## 各个部分的解释和修改
### util/generate_vocab_file.py
该函数是生成vocab_file的函数  
生成模式有三种：sentencepiece/ word/ character  
- sentencepiece  
使用sentencepiece模式 需要提供文本(input_file) 模型输出路径(output_file) 字典大小(vocab_size) 和 语言复杂度(character_coverage)  
使用sentencepiece库对输入文本进行处理 处理的结果为.model文件和.vocab文件 存放在输出路径上  
- word/character
word/character模式很简单 使用Counter进行计数 通过频率的降序进行sub-word的生成  
``` python
python generate_vocab_file.py --input_file ../../../../data_dir/DLHLP/text-data.txt --mode character --output_file './character.txt'
```
这样就生成了vocab_file.txt 放在ASR项目根目录下  
这个vocab_file.txt的组织形式是按照character的出现频率降序排列的  
见[vocab_file.txt](End-to-end-ASR-Pytorch-master/vocab_file.txt)  

### 根据数据创建sub-word字典 corpus/dlhlp.py corpus/librispeech.py
该项目默认是处理librispeech数据 当需要使用新的数据集时 仿照librispeech.py创建dlhlp.py  
在dlhlp.py中定义自定义数据的dataset 用于后续src/data使用  
dataset中使用多线程进行wav文件名到对应文本的映射  

### src/text.py 一套完备的文本处理模式
src/text.py中提供了一套完备的文本处理模式 对文本进行encode和decode  
1. 通过设置基类_BaseTextEncoder 其中定义了基本的encode decode方法 同时包含了vocab_size(词典大小) token_type(token的形式) pad_idx eos_idx unk_idx等常量  
2. CharacterTextEncode继承于基类 作为以字母为token的文本处理方式 其中通过数组实现idx_to_vocab 通过字典实现vocab_to_idx  
3. SubwordTextEncode继承于基类 作为以sub-word为token的文本处理方式 其中通过sentencepiece的函数进行encode decode
4. WordTextEncode继承于CharacterTextEncode 作为以word作为token的文本处理方式 实现方式类似于Character
5. BertTextEncode继承于基类 以transformers的BertTokenizer作为encode和decode的方式



